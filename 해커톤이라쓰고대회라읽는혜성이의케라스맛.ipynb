{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "해커톤이라쓰고대회라읽는혜성이의케라스맛",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllyHyeseongKim/2019_cau_oss_hackathon/blob/master/%ED%95%B4%EC%BB%A4%ED%86%A4%EC%9D%B4%EB%9D%BC%EC%93%B0%EA%B3%A0%EB%8C%80%ED%9A%8C%EB%9D%BC%EC%9D%BD%EB%8A%94%ED%98%9C%EC%84%B1%EC%9D%B4%EC%9D%98%EC%BC%80%EB%9D%BC%EC%8A%A4%EB%A7%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc",
        "colab_type": "text"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        " *  단, 사용할 데이터셋에 따라 is_mnist만 수정\n",
        "*   모든 구현은 [2. 데이터 전처리]와 [3. 모델 생성]에서만 진행\n",
        " *  데이터 전처리 후 트레이닝, 데이터 셋은 x_train_after, x_test_after 변수명을 유지해주세요.\n",
        " *  데이터셋이 달라져도 같은 모델 구조를 유지하여야함.\n",
        "*   [4. 모델 저장]과 [5. 모델 로드 및 평가]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *  트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        " *  team_name을 제외한 다른 부분은 수정하지 말 것\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  모델 구조 파일 (model_structure_teamXX.json)\n",
        " *  모델 weight 파일 (model_weight_teamXX.h5)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        "* 제출 기한: **오후 6시**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2019_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  두 개의 데이터셋에 대해 다른 모델 구조를 사용한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1",
        "colab_type": "text"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "is_mnist = False;\n",
        "\n",
        "# 데이터셋 로드\n",
        "# x_train, y_train: 트레이닝 데이터 및 레이블\n",
        "# x_test, y_test: 테스트 데이터 및 레이블\n",
        "if is_mnist:\n",
        "  data_type = 'mnist'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # fashion MNIST 데이터셋인 경우,\n",
        "else:\n",
        "  data_type = 'cifar10'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # cifar10 데이터셋인 경우,\n",
        "\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9c2KLDBIhNQ",
        "colab_type": "text"
      },
      "source": [
        "# **2. 데이터 전처리**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJNgjaHvIhSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0\n",
        "\n",
        "if is_mnist:\n",
        "  x_train_after = x_train_after.reshape(x_train_after.shape[0], 28, 28, 1)\n",
        "  x_test_after = x_test_after.reshape(x_test_after.shape[0], 28, 28, 1)\n",
        "  input_shape = x_test_after.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9",
        "colab_type": "text"
      },
      "source": [
        "# **3. 모델 생성**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZP4eRmRqgRp",
        "colab_type": "code",
        "outputId": "5ed5cf13-fa91-494f-a1a9-adb6b356dbfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#혜성\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 순차 모델 생성 (가장 기본구조)\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Convolution layer: 3 x 3 x 32 필터로 특징 추출\n",
        "# 활성화 함수 = relu\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# MaxPooling layer: 출력 크기를 입력 크기의 반으로\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "# Convolution layer: 3 x 3 x 64 필터로 특징 추출\n",
        "# 활성화 함수 = relu\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# MaxPooling layer: 출력 크기를 입력 크기의 반으로\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "# Flatten layer: 7 x 7 x 1 image를 3136개의 1D vector input으로 변환\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# 1st hidden layer: fully-connected layer\n",
        "# (# of inputs = 3136, # of outputs = 256, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(256, activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "# 2nd hidden layer: fully-connected layer \n",
        "# (# of inputs = 256, # of outputs = 10, actication fuction = softmax)\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# learning rate scheduler 정의\n",
        "# 30 epoch마다 learning rate을 0.1배\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    decay_rate = 0.1\n",
        "    decay_step = 90\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        return lr * decay_rate\n",
        "    return lr\n",
        "\n",
        "save_path = '/content/'\n",
        "team_name = 'team01'\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "\n",
        "# optimizer를 Adam으로 설정\n",
        "adam = optimizers.Adam(lr=0.001, decay=1e-5)\n",
        "model_path = save_path + 'model_entire_' + data_type + '_' + team_name + '.h5'\n",
        "\n",
        "# callback함수로 Checkpoint 객체 설정\n",
        "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "# callback함수 리스트 정의\n",
        "callbacks=[\n",
        "    cb_checkpoint, \n",
        "    LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "]\n",
        "\n",
        "# 모델 호츨 및 컴파일 \n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 128, epochs = 100, shuffle=True, validation_data=[x_test_after, y_test], callbacks=callbacks)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 1/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 1.6674 - acc: 0.3887\n",
            "Epoch 00001: val_acc improved from -inf to 0.52450, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 9s 175us/sample - loss: 1.6669 - acc: 0.3889 - val_loss: 1.2932 - val_acc: 0.5245\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 2/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 1.2651 - acc: 0.5471\n",
            "Epoch 00002: val_acc improved from 0.52450 to 0.60140, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 120us/sample - loss: 1.2651 - acc: 0.5471 - val_loss: 1.1206 - val_acc: 0.6014\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 3/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 1.0912 - acc: 0.6130\n",
            "Epoch 00003: val_acc improved from 0.60140 to 0.67250, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 1.0905 - acc: 0.6131 - val_loss: 0.9238 - val_acc: 0.6725\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 4/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.9680 - acc: 0.6595\n",
            "Epoch 00004: val_acc improved from 0.67250 to 0.68540, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.9678 - acc: 0.6596 - val_loss: 0.9039 - val_acc: 0.6854\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 5/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.8792 - acc: 0.6921\n",
            "Epoch 00005: val_acc improved from 0.68540 to 0.72990, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.8788 - acc: 0.6924 - val_loss: 0.7735 - val_acc: 0.7299\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 6/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.8140 - acc: 0.7134\n",
            "Epoch 00006: val_acc improved from 0.72990 to 0.73160, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 120us/sample - loss: 0.8138 - acc: 0.7135 - val_loss: 0.7647 - val_acc: 0.7316\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 7/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.7666 - acc: 0.7304\n",
            "Epoch 00007: val_acc improved from 0.73160 to 0.75110, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.7670 - acc: 0.7303 - val_loss: 0.7096 - val_acc: 0.7511\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 8/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.7210 - acc: 0.7448\n",
            "Epoch 00008: val_acc improved from 0.75110 to 0.76080, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.7207 - acc: 0.7448 - val_loss: 0.6957 - val_acc: 0.7608\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 9/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.7571\n",
            "Epoch 00009: val_acc improved from 0.76080 to 0.76780, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.6929 - acc: 0.7569 - val_loss: 0.6751 - val_acc: 0.7678\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 10/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.6581 - acc: 0.7684\n",
            "Epoch 00010: val_acc improved from 0.76780 to 0.78010, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.6580 - acc: 0.7684 - val_loss: 0.6539 - val_acc: 0.7801\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 11/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.7748\n",
            "Epoch 00011: val_acc did not improve from 0.78010\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.6356 - acc: 0.7748 - val_loss: 0.6577 - val_acc: 0.7731\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 12/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.7857\n",
            "Epoch 00012: val_acc did not improve from 0.78010\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.6022 - acc: 0.7857 - val_loss: 0.6337 - val_acc: 0.7784\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 13/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.7962\n",
            "Epoch 00013: val_acc did not improve from 0.78010\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.5793 - acc: 0.7961 - val_loss: 0.6604 - val_acc: 0.7764\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 14/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5675 - acc: 0.7998\n",
            "Epoch 00014: val_acc improved from 0.78010 to 0.78550, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.5671 - acc: 0.8000 - val_loss: 0.6294 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 15/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8070\n",
            "Epoch 00015: val_acc improved from 0.78550 to 0.78580, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.5442 - acc: 0.8071 - val_loss: 0.6234 - val_acc: 0.7858\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 16/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8137\n",
            "Epoch 00016: val_acc improved from 0.78580 to 0.78790, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.5230 - acc: 0.8139 - val_loss: 0.6272 - val_acc: 0.7879\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 17/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.8157\n",
            "Epoch 00017: val_acc improved from 0.78790 to 0.78960, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.5176 - acc: 0.8157 - val_loss: 0.6199 - val_acc: 0.7896\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 18/100\n",
            "49536/50000 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.8248\n",
            "Epoch 00018: val_acc improved from 0.78960 to 0.79540, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.4923 - acc: 0.8248 - val_loss: 0.6078 - val_acc: 0.7954\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 19/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.4814 - acc: 0.8290\n",
            "Epoch 00019: val_acc did not improve from 0.79540\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.4809 - acc: 0.8291 - val_loss: 0.6185 - val_acc: 0.7931\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 20/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8316\n",
            "Epoch 00020: val_acc improved from 0.79540 to 0.79790, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.4711 - acc: 0.8317 - val_loss: 0.6229 - val_acc: 0.7979\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 21/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.8365\n",
            "Epoch 00021: val_acc improved from 0.79790 to 0.80220, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.4616 - acc: 0.8366 - val_loss: 0.6022 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 22/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8395\n",
            "Epoch 00022: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.4493 - acc: 0.8395 - val_loss: 0.6272 - val_acc: 0.7923\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 23/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8419\n",
            "Epoch 00023: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.4430 - acc: 0.8418 - val_loss: 0.6365 - val_acc: 0.7891\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 24/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8488\n",
            "Epoch 00024: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.4239 - acc: 0.8488 - val_loss: 0.6453 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 25/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8527\n",
            "Epoch 00025: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.4112 - acc: 0.8530 - val_loss: 0.6234 - val_acc: 0.7951\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 26/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8555\n",
            "Epoch 00026: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.4042 - acc: 0.8556 - val_loss: 0.6357 - val_acc: 0.7954\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 27/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8576\n",
            "Epoch 00027: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3935 - acc: 0.8577 - val_loss: 0.6237 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 28/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8612\n",
            "Epoch 00028: val_acc did not improve from 0.80220\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3902 - acc: 0.8612 - val_loss: 0.6197 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 29/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8641\n",
            "Epoch 00029: val_acc improved from 0.80220 to 0.80290, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.3809 - acc: 0.8640 - val_loss: 0.6157 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 30/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8654\n",
            "Epoch 00030: val_acc did not improve from 0.80290\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3714 - acc: 0.8655 - val_loss: 0.6274 - val_acc: 0.8002\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 31/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8681\n",
            "Epoch 00031: val_acc did not improve from 0.80290\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3693 - acc: 0.8681 - val_loss: 0.6278 - val_acc: 0.7989\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 32/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8709\n",
            "Epoch 00032: val_acc did not improve from 0.80290\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3581 - acc: 0.8709 - val_loss: 0.6531 - val_acc: 0.8010\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 33/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8731\n",
            "Epoch 00033: val_acc improved from 0.80290 to 0.80400, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.3490 - acc: 0.8732 - val_loss: 0.6326 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 34/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8756\n",
            "Epoch 00034: val_acc did not improve from 0.80400\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3442 - acc: 0.8756 - val_loss: 0.6584 - val_acc: 0.7976\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 35/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8751\n",
            "Epoch 00035: val_acc did not improve from 0.80400\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.3452 - acc: 0.8751 - val_loss: 0.6422 - val_acc: 0.8006\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 36/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8794\n",
            "Epoch 00036: val_acc did not improve from 0.80400\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.3338 - acc: 0.8792 - val_loss: 0.6624 - val_acc: 0.8038\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 37/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8823\n",
            "Epoch 00037: val_acc did not improve from 0.80400\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3263 - acc: 0.8823 - val_loss: 0.6418 - val_acc: 0.7993\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 38/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8834\n",
            "Epoch 00038: val_acc improved from 0.80400 to 0.80420, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.3258 - acc: 0.8833 - val_loss: 0.6387 - val_acc: 0.8042\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 39/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8843\n",
            "Epoch 00039: val_acc did not improve from 0.80420\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3211 - acc: 0.8842 - val_loss: 0.6716 - val_acc: 0.7978\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 40/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8879\n",
            "Epoch 00040: val_acc improved from 0.80420 to 0.80680, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.3111 - acc: 0.8878 - val_loss: 0.6375 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 41/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8868\n",
            "Epoch 00041: val_acc did not improve from 0.80680\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3145 - acc: 0.8869 - val_loss: 0.6619 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 42/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.8914\n",
            "Epoch 00042: val_acc did not improve from 0.80680\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3059 - acc: 0.8913 - val_loss: 0.6560 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 43/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8874\n",
            "Epoch 00043: val_acc did not improve from 0.80680\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3111 - acc: 0.8875 - val_loss: 0.6581 - val_acc: 0.8045\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 44/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.8895\n",
            "Epoch 00044: val_acc did not improve from 0.80680\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.3089 - acc: 0.8898 - val_loss: 0.6496 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 45/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.8946\n",
            "Epoch 00045: val_acc improved from 0.80680 to 0.80770, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.6768 - val_acc: 0.8077\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 46/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.8956\n",
            "Epoch 00046: val_acc did not improve from 0.80770\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.2891 - acc: 0.8957 - val_loss: 0.6499 - val_acc: 0.8077\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 47/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.8987\n",
            "Epoch 00047: val_acc did not improve from 0.80770\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.2855 - acc: 0.8986 - val_loss: 0.6755 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 48/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.8980\n",
            "Epoch 00048: val_acc did not improve from 0.80770\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2859 - acc: 0.8980 - val_loss: 0.6792 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 49/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.8984\n",
            "Epoch 00049: val_acc did not improve from 0.80770\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2815 - acc: 0.8984 - val_loss: 0.6763 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 50/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9015\n",
            "Epoch 00050: val_acc improved from 0.80770 to 0.80890, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2755 - acc: 0.9014 - val_loss: 0.6853 - val_acc: 0.8089\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 51/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9011\n",
            "Epoch 00051: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.2784 - acc: 0.9012 - val_loss: 0.6680 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 52/100\n",
            "49536/50000 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9038\n",
            "Epoch 00052: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.2691 - acc: 0.9037 - val_loss: 0.6938 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 53/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9050\n",
            "Epoch 00053: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2662 - acc: 0.9049 - val_loss: 0.6936 - val_acc: 0.8061\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 54/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9036\n",
            "Epoch 00054: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 116us/sample - loss: 0.2687 - acc: 0.9037 - val_loss: 0.6819 - val_acc: 0.8062\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 55/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9074\n",
            "Epoch 00055: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2611 - acc: 0.9074 - val_loss: 0.7120 - val_acc: 0.8046\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 56/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9039\n",
            "Epoch 00056: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2668 - acc: 0.9038 - val_loss: 0.6554 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 57/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9088\n",
            "Epoch 00057: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2569 - acc: 0.9088 - val_loss: 0.6655 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 58/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9085\n",
            "Epoch 00058: val_acc did not improve from 0.80890\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2551 - acc: 0.9084 - val_loss: 0.6893 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 59/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9092\n",
            "Epoch 00059: val_acc improved from 0.80890 to 0.81180, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.2550 - acc: 0.9091 - val_loss: 0.6796 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 60/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9118\n",
            "Epoch 00060: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2483 - acc: 0.9115 - val_loss: 0.6630 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 61/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9106\n",
            "Epoch 00061: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 116us/sample - loss: 0.2491 - acc: 0.9106 - val_loss: 0.6857 - val_acc: 0.8039\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 62/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9133\n",
            "Epoch 00062: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2462 - acc: 0.9132 - val_loss: 0.6936 - val_acc: 0.8010\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 63/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9111\n",
            "Epoch 00063: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2439 - acc: 0.9111 - val_loss: 0.6973 - val_acc: 0.7996\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 64/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9108\n",
            "Epoch 00064: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2497 - acc: 0.9108 - val_loss: 0.6829 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 65/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9149\n",
            "Epoch 00065: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2352 - acc: 0.9149 - val_loss: 0.7251 - val_acc: 0.8038\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 66/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9148\n",
            "Epoch 00066: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2363 - acc: 0.9149 - val_loss: 0.7080 - val_acc: 0.8048\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 67/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9165\n",
            "Epoch 00067: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2354 - acc: 0.9163 - val_loss: 0.6835 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 68/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9203\n",
            "Epoch 00068: val_acc did not improve from 0.81180\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2291 - acc: 0.9203 - val_loss: 0.7121 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 69/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9160\n",
            "Epoch 00069: val_acc improved from 0.81180 to 0.81240, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2374 - acc: 0.9161 - val_loss: 0.6673 - val_acc: 0.8124\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 70/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9170\n",
            "Epoch 00070: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2309 - acc: 0.9169 - val_loss: 0.7150 - val_acc: 0.8107\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 71/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9163\n",
            "Epoch 00071: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2360 - acc: 0.9162 - val_loss: 0.6930 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 72/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9198\n",
            "Epoch 00072: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2259 - acc: 0.9199 - val_loss: 0.6952 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 73/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9203\n",
            "Epoch 00073: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2246 - acc: 0.9203 - val_loss: 0.7440 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 74/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9199\n",
            "Epoch 00074: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2230 - acc: 0.9199 - val_loss: 0.7208 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 75/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9233\n",
            "Epoch 00075: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2156 - acc: 0.9233 - val_loss: 0.7236 - val_acc: 0.8072\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 76/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9220\n",
            "Epoch 00076: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 116us/sample - loss: 0.2176 - acc: 0.9219 - val_loss: 0.7327 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 77/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9238\n",
            "Epoch 00077: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2169 - acc: 0.9240 - val_loss: 0.7059 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 78/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9215\n",
            "Epoch 00078: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2221 - acc: 0.9214 - val_loss: 0.6892 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 79/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9215\n",
            "Epoch 00079: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2178 - acc: 0.9215 - val_loss: 0.7238 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 80/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9217\n",
            "Epoch 00080: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2203 - acc: 0.9217 - val_loss: 0.7206 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 81/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9246\n",
            "Epoch 00081: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2154 - acc: 0.9245 - val_loss: 0.7313 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 82/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9255\n",
            "Epoch 00082: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2109 - acc: 0.9255 - val_loss: 0.7042 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 83/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9265\n",
            "Epoch 00083: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2069 - acc: 0.9265 - val_loss: 0.7333 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 84/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9288\n",
            "Epoch 00084: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2020 - acc: 0.9287 - val_loss: 0.7263 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 85/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9268\n",
            "Epoch 00085: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2067 - acc: 0.9268 - val_loss: 0.7785 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 86/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9245\n",
            "Epoch 00086: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2086 - acc: 0.9245 - val_loss: 0.7428 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 87/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9278\n",
            "Epoch 00087: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2029 - acc: 0.9279 - val_loss: 0.7159 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 88/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9266\n",
            "Epoch 00088: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.2039 - acc: 0.9266 - val_loss: 0.7132 - val_acc: 0.8114\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 89/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9282\n",
            "Epoch 00089: val_acc did not improve from 0.81240\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.2007 - acc: 0.9282 - val_loss: 0.7628 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 90/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9301\n",
            "Epoch 00090: val_acc improved from 0.81240 to 0.81250, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.1974 - acc: 0.9301 - val_loss: 0.7133 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 91/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9401\n",
            "Epoch 00091: val_acc improved from 0.81250 to 0.81450, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 120us/sample - loss: 0.1687 - acc: 0.9402 - val_loss: 0.7267 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 92/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9436\n",
            "Epoch 00092: val_acc improved from 0.81450 to 0.81520, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.1573 - acc: 0.9434 - val_loss: 0.7291 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 93/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9469\n",
            "Epoch 00093: val_acc improved from 0.81520 to 0.81740, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 119us/sample - loss: 0.1491 - acc: 0.9468 - val_loss: 0.7324 - val_acc: 0.8174\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 94/100\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9479\n",
            "Epoch 00094: val_acc did not improve from 0.81740\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.1473 - acc: 0.9478 - val_loss: 0.7389 - val_acc: 0.8173\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 95/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9474\n",
            "Epoch 00095: val_acc improved from 0.81740 to 0.81810, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.1471 - acc: 0.9474 - val_loss: 0.7365 - val_acc: 0.8181\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 96/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9478\n",
            "Epoch 00096: val_acc did not improve from 0.81810\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.1474 - acc: 0.9478 - val_loss: 0.7357 - val_acc: 0.8178\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 97/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9487\n",
            "Epoch 00097: val_acc did not improve from 0.81810\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.1457 - acc: 0.9487 - val_loss: 0.7396 - val_acc: 0.8174\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 98/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9512\n",
            "Epoch 00098: val_acc improved from 0.81810 to 0.81830, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.1380 - acc: 0.9513 - val_loss: 0.7475 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 99/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9499\n",
            "Epoch 00099: val_acc improved from 0.81830 to 0.81940, saving model to /content/checkpoint_cifar10_team01.h5\n",
            "50000/50000 [==============================] - 6s 118us/sample - loss: 0.1390 - acc: 0.9499 - val_loss: 0.7453 - val_acc: 0.8194\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 100/100\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9491\n",
            "Epoch 00100: val_acc did not improve from 0.81940\n",
            "50000/50000 [==============================] - 6s 117us/sample - loss: 0.1413 - acc: 0.9490 - val_loss: 0.7454 - val_acc: 0.8188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbf1a0cb550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR",
        "colab_type": "text"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team01'\n",
        "\n",
        "# 모델의 weight 값만 저장합니다.\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2BoRDZ7cFl",
        "colab_type": "text"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDBwxVUx7knQ",
        "colab_type": "code",
        "outputId": "bfe4dec2-0038-4d98-80cd-3e3347c58bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team01'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 145us/sample - loss: 0.7453 - acc: 0.8194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7452802639961242, 0.8194]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}